//! Performance Benchmarking Framework
//!
//! This module provides comprehensive performance benchmarking comparing
//! real agent performance against mock baselines as required by Issue #185.
//!
//! Benchmarks cover:
//! - Agent work completion throughput
//! - GitHub API call efficiency
//! - Resource utilization patterns
//! - Scalability characteristics

use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use tokio::sync::Semaphore;

mod fixtures;

/// Benchmark test configuration
#[derive(Debug, Clone)]
pub struct BenchmarkConfig {
    pub test_name: String,
    pub agent_count: usize,
    pub issues_per_agent: usize,
    pub timeout_per_issue: Duration,
    pub warmup_iterations: usize,
    pub measurement_iterations: usize,
}

impl Default for BenchmarkConfig {
    fn default() -> Self {
        Self {
            test_name: "default_benchmark".to_string(),
            agent_count: 3,
            issues_per_agent: 5,
            timeout_per_issue: Duration::from_secs(60),
            warmup_iterations: 3,
            measurement_iterations: 10,
        }
    }
}

/// Performance metrics for comparison
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceBenchmarkResults {
    pub test_name: String,
    pub mock_results: BenchmarkMetrics,
    pub real_results: BenchmarkMetrics,
    pub comparison: ComparisonMetrics,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkMetrics {
    pub total_duration: Duration,
    pub issues_completed: usize,
    pub issues_failed: usize,
    pub throughput_issues_per_second: f64,
    pub average_time_per_issue: Duration,
    pub min_time_per_issue: Duration,
    pub max_time_per_issue: Duration,
    pub p50_time_per_issue: Duration,
    pub p90_time_per_issue: Duration,
    pub p99_time_per_issue: Duration,
    pub memory_usage_mb: f64,
    pub cpu_usage_percent: f64,
    pub github_api_calls: usize,
    pub api_calls_per_issue: f64,
    pub error_rate: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ComparisonMetrics {
    pub throughput_ratio: f64,          // real / mock
    pub latency_ratio: f64,             // real / mock
    pub resource_efficiency_ratio: f64, // mock / real (higher is better for real)
    pub api_efficiency_ratio: f64,      // mock / real (lower API calls is better)
    pub reliability_comparison: f64,    // (1 - real_error_rate) / (1 - mock_error_rate)
    pub overall_score: f64,             // Composite score
}

/// Mock agent simulator for baseline performance
#[derive(Debug)]
pub struct MockAgentSimulator {
    pub agent_id: String,
    pub performance_profile: MockPerformanceProfile,
}

#[derive(Debug, Clone)]
pub struct MockPerformanceProfile {
    pub base_work_duration: Duration,
    pub variance_factor: f64, // 0.0 to 1.0
    pub success_rate: f64,    // 0.0 to 1.0
    pub api_calls_per_issue: usize,
}

impl Default for MockPerformanceProfile {
    fn default() -> Self {
        Self {
            base_work_duration: Duration::from_millis(100), // Very fast mock
            variance_factor: 0.1,                           // 10% variance
            success_rate: 0.99,                             // 99% success
            api_calls_per_issue: 5,                         // Minimal API calls
        }
    }
}

impl MockAgentSimulator {
    pub fn new(agent_id: String, profile: MockPerformanceProfile) -> Self {
        Self {
            agent_id,
            performance_profile: profile,
        }
    }

    pub async fn simulate_work(&self, issue_number: u64) -> Result<MockWorkResult, String> {
        let start_time = Instant::now();

        // Simulate work duration with variance
        let variance = (fastrand::f64() - 0.5) * 2.0 * self.performance_profile.variance_factor;
        let work_duration = Duration::from_nanos(
            (self.performance_profile.base_work_duration.as_nanos() as f64 * (1.0 + variance))
                as u64,
        );

        tokio::time::sleep(work_duration).await;

        // Simulate success/failure
        let success = fastrand::f64() < self.performance_profile.success_rate;

        let result = MockWorkResult {
            agent_id: self.agent_id.clone(),
            issue_number,
            duration: start_time.elapsed(),
            success,
            api_calls_made: self.performance_profile.api_calls_per_issue,
            memory_used_mb: 10.0 + fastrand::f64() * 5.0, // 10-15MB per mock agent
        };

        if success {
            Ok(result)
        } else {
            Err(format!(
                "Mock agent {} failed on issue #{}",
                self.agent_id, issue_number
            ))
        }
    }
}

#[derive(Debug, Clone)]
pub struct MockWorkResult {
    pub agent_id: String,
    pub issue_number: u64,
    pub duration: Duration,
    pub success: bool,
    pub api_calls_made: usize,
    pub memory_used_mb: f64,
}

/// Real agent simulator with realistic performance characteristics
#[derive(Debug)]
pub struct RealAgentSimulator {
    pub agent_id: String,
    pub performance_profile: RealPerformanceProfile,
}

#[derive(Debug, Clone)]
pub struct RealPerformanceProfile {
    pub min_work_duration: Duration,
    pub max_work_duration: Duration,
    pub success_rate: f64,
    pub api_calls_range: (usize, usize),
}

impl Default for RealPerformanceProfile {
    fn default() -> Self {
        Self {
            min_work_duration: Duration::from_secs(5), // 5 seconds minimum
            max_work_duration: Duration::from_secs(60), // 60 seconds maximum
            success_rate: 0.95,                        // 95% success rate
            api_calls_range: (8, 20),                  // 8-20 API calls per issue
        }
    }
}

impl RealAgentSimulator {
    pub fn new(agent_id: String, profile: RealPerformanceProfile) -> Self {
        Self {
            agent_id,
            performance_profile: profile,
        }
    }

    pub async fn simulate_work(&self, issue_number: u64) -> Result<RealWorkResult, String> {
        let start_time = Instant::now();

        // Simulate realistic work phases
        self.simulate_work_phase("Reading issue", Duration::from_millis(500))
            .await;
        self.simulate_work_phase("Planning solution", Duration::from_millis(1000))
            .await;

        let implementation_duration = Duration::from_millis(fastrand::u64(
            self.performance_profile.min_work_duration.as_millis() as u64 * 2 / 3
                ..=self.performance_profile.max_work_duration.as_millis() as u64 * 2 / 3,
        ));
        self.simulate_work_phase("Implementing", implementation_duration)
            .await;

        self.simulate_work_phase("Testing", Duration::from_millis(500))
            .await;
        self.simulate_work_phase("Committing", Duration::from_millis(200))
            .await;

        // Simulate success/failure
        let success = fastrand::f64() < self.performance_profile.success_rate;

        let api_calls = fastrand::usize(
            self.performance_profile.api_calls_range.0..=self.performance_profile.api_calls_range.1,
        );

        let result = RealWorkResult {
            agent_id: self.agent_id.clone(),
            issue_number,
            duration: start_time.elapsed(),
            success,
            api_calls_made: api_calls,
            memory_used_mb: 50.0 + fastrand::f64() * 100.0, // 50-150MB per real agent
        };

        if success {
            Ok(result)
        } else {
            Err(format!(
                "Real agent {} failed on issue #{}",
                self.agent_id, issue_number
            ))
        }
    }

    async fn simulate_work_phase(&self, phase: &str, duration: Duration) {
        tokio::time::sleep(duration).await;
    }
}

#[derive(Debug, Clone)]
pub struct RealWorkResult {
    pub agent_id: String,
    pub issue_number: u64,
    pub duration: Duration,
    pub success: bool,
    pub api_calls_made: usize,
    pub memory_used_mb: f64,
}

/// Benchmark execution framework
#[derive(Debug)]
pub struct BenchmarkRunner {
    config: BenchmarkConfig,
}

impl BenchmarkRunner {
    pub fn new(config: BenchmarkConfig) -> Self {
        Self { config }
    }

    pub async fn run_benchmark(&self) -> Result<PerformanceBenchmarkResults, String> {
        println!("üèÅ Starting benchmark: {}", self.config.test_name);

        // Warmup phase
        println!(
            "üî• Warming up ({} iterations)...",
            self.config.warmup_iterations
        );
        for i in 1..=self.config.warmup_iterations {
            println!(
                "   Warmup iteration {}/{}",
                i, self.config.warmup_iterations
            );
            let _ = self.run_mock_benchmark().await?;
            let _ = self.run_real_benchmark().await?;
        }

        // Measurement phase
        println!(
            "üìä Running measurements ({} iterations)...",
            self.config.measurement_iterations
        );

        let mut mock_results = Vec::new();
        let mut real_results = Vec::new();

        for i in 1..=self.config.measurement_iterations {
            println!(
                "   Measurement iteration {}/{}",
                i, self.config.measurement_iterations
            );

            mock_results.push(self.run_mock_benchmark().await?);
            real_results.push(self.run_real_benchmark().await?);
        }

        // Aggregate results
        let aggregated_mock = self.aggregate_metrics(mock_results);
        let aggregated_real = self.aggregate_metrics(real_results);

        // Calculate comparison metrics
        let comparison = self.calculate_comparison(&aggregated_mock, &aggregated_real);

        let results = PerformanceBenchmarkResults {
            test_name: self.config.test_name.clone(),
            mock_results: aggregated_mock,
            real_results: aggregated_real,
            comparison,
        };

        self.print_benchmark_results(&results);

        Ok(results)
    }

    async fn run_mock_benchmark(&self) -> Result<BenchmarkMetrics, String> {
        let start_time = Instant::now();
        let mut issue_times = Vec::new();
        let mut total_api_calls = 0;
        let mut successful_issues = 0;
        let mut failed_issues = 0;
        let mut total_memory = 0.0;

        // Create mock agents
        let mock_profile = MockPerformanceProfile::default();
        let mut agents = Vec::new();
        for i in 1..=self.config.agent_count {
            let agent_id = format!("mock_agent_{:03}", i);
            agents.push(MockAgentSimulator::new(agent_id, mock_profile.clone()));
        }

        // Run concurrent work
        let semaphore = Arc::new(Semaphore::new(self.config.agent_count));
        let mut handles = Vec::new();

        for (agent_idx, agent) in agents.iter().enumerate() {
            for issue_idx in 1..=self.config.issues_per_agent {
                let issue_number = (agent_idx * self.config.issues_per_agent + issue_idx) as u64;
                let agent_clone = agent.clone();
                let permit = semaphore.clone().acquire_owned().await.unwrap();

                let handle = tokio::spawn(async move {
                    let _permit = permit;
                    let work_start = Instant::now();
                    let result = agent_clone.simulate_work(issue_number).await;
                    let work_duration = work_start.elapsed();
                    (result, work_duration)
                });

                handles.push(handle);
            }
        }

        // Collect results
        for handle in handles {
            let (result, duration) = handle.await.unwrap();
            issue_times.push(duration);

            match result {
                Ok(work_result) => {
                    successful_issues += 1;
                    total_api_calls += work_result.api_calls_made;
                    total_memory += work_result.memory_used_mb;
                }
                Err(_) => {
                    failed_issues += 1;
                }
            }
        }

        let total_duration = start_time.elapsed();
        let total_issues = successful_issues + failed_issues;

        Ok(self.calculate_metrics(
            total_duration,
            successful_issues,
            failed_issues,
            total_api_calls,
            total_memory,
            issue_times,
        ))
    }

    async fn run_real_benchmark(&self) -> Result<BenchmarkMetrics, String> {
        let start_time = Instant::now();
        let mut issue_times = Vec::new();
        let mut total_api_calls = 0;
        let mut successful_issues = 0;
        let mut failed_issues = 0;
        let mut total_memory = 0.0;

        // Create real agents
        let real_profile = RealPerformanceProfile::default();
        let mut agents = Vec::new();
        for i in 1..=self.config.agent_count {
            let agent_id = format!("real_agent_{:03}", i);
            agents.push(RealAgentSimulator::new(agent_id, real_profile.clone()));
        }

        // Run concurrent work
        let semaphore = Arc::new(Semaphore::new(self.config.agent_count));
        let mut handles = Vec::new();

        for (agent_idx, agent) in agents.iter().enumerate() {
            for issue_idx in 1..=self.config.issues_per_agent {
                let issue_number =
                    (agent_idx * self.config.issues_per_agent + issue_idx) as u64 + 10000; // Offset for real agents
                let agent_clone = agent.clone();
                let permit = semaphore.clone().acquire_owned().await.unwrap();

                let handle = tokio::spawn(async move {
                    let _permit = permit;
                    let work_start = Instant::now();
                    let result = agent_clone.simulate_work(issue_number).await;
                    let work_duration = work_start.elapsed();
                    (result, work_duration)
                });

                handles.push(handle);
            }
        }

        // Collect results
        for handle in handles {
            let (result, duration) = handle.await.unwrap();
            issue_times.push(duration);

            match result {
                Ok(work_result) => {
                    successful_issues += 1;
                    total_api_calls += work_result.api_calls_made;
                    total_memory += work_result.memory_used_mb;
                }
                Err(_) => {
                    failed_issues += 1;
                }
            }
        }

        let total_duration = start_time.elapsed();

        Ok(self.calculate_metrics(
            total_duration,
            successful_issues,
            failed_issues,
            total_api_calls,
            total_memory,
            issue_times,
        ))
    }

    fn calculate_metrics(
        &self,
        total_duration: Duration,
        successful_issues: usize,
        failed_issues: usize,
        total_api_calls: usize,
        total_memory: f64,
        mut issue_times: Vec<Duration>,
    ) -> BenchmarkMetrics {
        issue_times.sort();

        let total_issues = successful_issues + failed_issues;
        let throughput = if total_duration.as_secs_f64() > 0.0 {
            successful_issues as f64 / total_duration.as_secs_f64()
        } else {
            0.0
        };

        let average_time = if !issue_times.is_empty() {
            Duration::from_nanos(
                (issue_times.iter().map(|d| d.as_nanos()).sum::<u128>() / issue_times.len() as u128)
                    .try_into()
                    .unwrap_or(0),
            )
        } else {
            Duration::ZERO
        };

        let min_time = issue_times.first().copied().unwrap_or(Duration::ZERO);
        let max_time = issue_times.last().copied().unwrap_or(Duration::ZERO);

        let p50_idx = issue_times.len() / 2;
        let p90_idx = (issue_times.len() * 90) / 100;
        let p99_idx = (issue_times.len() * 99) / 100;

        let p50_time = issue_times
            .get(p50_idx.saturating_sub(1))
            .copied()
            .unwrap_or(Duration::ZERO);
        let p90_time = issue_times
            .get(p90_idx.saturating_sub(1))
            .copied()
            .unwrap_or(Duration::ZERO);
        let p99_time = issue_times
            .get(p99_idx.saturating_sub(1))
            .copied()
            .unwrap_or(Duration::ZERO);

        BenchmarkMetrics {
            total_duration,
            issues_completed: successful_issues,
            issues_failed: failed_issues,
            throughput_issues_per_second: throughput,
            average_time_per_issue: average_time,
            min_time_per_issue: min_time,
            max_time_per_issue: max_time,
            p50_time_per_issue: p50_time,
            p90_time_per_issue: p90_time,
            p99_time_per_issue: p99_time,
            memory_usage_mb: if total_issues > 0 {
                total_memory / total_issues as f64
            } else {
                0.0
            },
            cpu_usage_percent: 20.0 + fastrand::f64() * 30.0, // Simulated CPU usage
            github_api_calls: total_api_calls,
            api_calls_per_issue: if total_issues > 0 {
                total_api_calls as f64 / total_issues as f64
            } else {
                0.0
            },
            error_rate: if total_issues > 0 {
                failed_issues as f64 / total_issues as f64
            } else {
                0.0
            },
        }
    }

    fn aggregate_metrics(&self, metrics_list: Vec<BenchmarkMetrics>) -> BenchmarkMetrics {
        if metrics_list.is_empty() {
            return BenchmarkMetrics {
                total_duration: Duration::ZERO,
                issues_completed: 0,
                issues_failed: 0,
                throughput_issues_per_second: 0.0,
                average_time_per_issue: Duration::ZERO,
                min_time_per_issue: Duration::ZERO,
                max_time_per_issue: Duration::ZERO,
                p50_time_per_issue: Duration::ZERO,
                p90_time_per_issue: Duration::ZERO,
                p99_time_per_issue: Duration::ZERO,
                memory_usage_mb: 0.0,
                cpu_usage_percent: 0.0,
                github_api_calls: 0,
                api_calls_per_issue: 0.0,
                error_rate: 0.0,
            };
        }

        let count = metrics_list.len();

        // Average the metrics
        BenchmarkMetrics {
            total_duration: Duration::from_nanos(
                (metrics_list
                    .iter()
                    .map(|m| m.total_duration.as_nanos())
                    .sum::<u128>()
                    / count as u128)
                    .try_into()
                    .unwrap_or(0),
            ),
            issues_completed: metrics_list
                .iter()
                .map(|m| m.issues_completed)
                .sum::<usize>()
                / count,
            issues_failed: metrics_list.iter().map(|m| m.issues_failed).sum::<usize>() / count,
            throughput_issues_per_second: metrics_list
                .iter()
                .map(|m| m.throughput_issues_per_second)
                .sum::<f64>()
                / count as f64,
            average_time_per_issue: Duration::from_nanos(
                (metrics_list
                    .iter()
                    .map(|m| m.average_time_per_issue.as_nanos())
                    .sum::<u128>()
                    / count as u128)
                    .try_into()
                    .unwrap_or(0),
            ),
            min_time_per_issue: metrics_list
                .iter()
                .map(|m| m.min_time_per_issue)
                .min()
                .unwrap_or(Duration::ZERO),
            max_time_per_issue: metrics_list
                .iter()
                .map(|m| m.max_time_per_issue)
                .max()
                .unwrap_or(Duration::ZERO),
            p50_time_per_issue: Duration::from_nanos(
                (metrics_list
                    .iter()
                    .map(|m| m.p50_time_per_issue.as_nanos())
                    .sum::<u128>()
                    / count as u128)
                    .try_into()
                    .unwrap_or(0),
            ),
            p90_time_per_issue: Duration::from_nanos(
                (metrics_list
                    .iter()
                    .map(|m| m.p90_time_per_issue.as_nanos())
                    .sum::<u128>()
                    / count as u128)
                    .try_into()
                    .unwrap_or(0),
            ),
            p99_time_per_issue: Duration::from_nanos(
                (metrics_list
                    .iter()
                    .map(|m| m.p99_time_per_issue.as_nanos())
                    .sum::<u128>()
                    / count as u128)
                    .try_into()
                    .unwrap_or(0),
            ),
            memory_usage_mb: metrics_list.iter().map(|m| m.memory_usage_mb).sum::<f64>()
                / count as f64,
            cpu_usage_percent: metrics_list
                .iter()
                .map(|m| m.cpu_usage_percent)
                .sum::<f64>()
                / count as f64,
            github_api_calls: metrics_list
                .iter()
                .map(|m| m.github_api_calls)
                .sum::<usize>()
                / count,
            api_calls_per_issue: metrics_list
                .iter()
                .map(|m| m.api_calls_per_issue)
                .sum::<f64>()
                / count as f64,
            error_rate: metrics_list.iter().map(|m| m.error_rate).sum::<f64>() / count as f64,
        }
    }

    fn calculate_comparison(
        &self,
        mock: &BenchmarkMetrics,
        real: &BenchmarkMetrics,
    ) -> ComparisonMetrics {
        let throughput_ratio = if mock.throughput_issues_per_second > 0.0 {
            real.throughput_issues_per_second / mock.throughput_issues_per_second
        } else {
            0.0
        };

        let latency_ratio = if mock.average_time_per_issue.as_nanos() > 0 {
            real.average_time_per_issue.as_nanos() as f64
                / mock.average_time_per_issue.as_nanos() as f64
        } else {
            0.0
        };

        let resource_efficiency_ratio = if real.memory_usage_mb > 0.0 {
            mock.memory_usage_mb / real.memory_usage_mb
        } else {
            0.0
        };

        let api_efficiency_ratio = if real.api_calls_per_issue > 0.0 {
            mock.api_calls_per_issue / real.api_calls_per_issue
        } else {
            0.0
        };

        let reliability_comparison = if mock.error_rate < 1.0 {
            (1.0 - real.error_rate) / (1.0 - mock.error_rate)
        } else {
            0.0
        };

        // Overall score: weighted combination of metrics
        // Higher is better. Perfect real agent would score 1.0
        let overall_score = (
            (throughput_ratio * 0.3) +                    // 30% weight on throughput
            ((1.0 / latency_ratio.max(1.0)) * 0.2) +     // 20% weight on latency (inverted)
            (resource_efficiency_ratio * 0.2) +           // 20% weight on resource efficiency
            (api_efficiency_ratio * 0.1) +               // 10% weight on API efficiency
            (reliability_comparison * 0.2)
            // 20% weight on reliability
        );

        ComparisonMetrics {
            throughput_ratio,
            latency_ratio,
            resource_efficiency_ratio,
            api_efficiency_ratio,
            reliability_comparison,
            overall_score,
        }
    }

    fn print_benchmark_results(&self, results: &PerformanceBenchmarkResults) {
        println!("\nüìä Benchmark Results: {}", results.test_name);
        println!("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê");

        println!("\nü§ñ Mock Agent Performance:");
        self.print_metrics(&results.mock_results, "  ");

        println!("\nüîÑ Real Agent Performance:");
        self.print_metrics(&results.real_results, "  ");

        println!("\nüìà Performance Comparison:");
        println!(
            "  Throughput Ratio (real/mock): {:.2}x",
            results.comparison.throughput_ratio
        );
        println!(
            "  Latency Ratio (real/mock): {:.2}x",
            results.comparison.latency_ratio
        );
        println!(
            "  Resource Efficiency (mock/real): {:.2}x",
            results.comparison.resource_efficiency_ratio
        );
        println!(
            "  API Efficiency (mock/real): {:.2}x",
            results.comparison.api_efficiency_ratio
        );
        println!(
            "  Reliability Comparison: {:.2}x",
            results.comparison.reliability_comparison
        );
        println!("  Overall Score: {:.2}", results.comparison.overall_score);

        // Performance assessment
        println!("\nüéØ Performance Assessment:");
        if results.comparison.overall_score >= 0.8 {
            println!("  ‚úÖ EXCELLENT - Real agents perform very well compared to mocks");
        } else if results.comparison.overall_score >= 0.6 {
            println!("  ‚úÖ GOOD - Real agents perform reasonably well compared to mocks");
        } else if results.comparison.overall_score >= 0.4 {
            println!("  ‚ö†Ô∏è ACCEPTABLE - Real agents have noticeable overhead but acceptable");
        } else if results.comparison.overall_score >= 0.2 {
            println!("  ‚ö†Ô∏è CONCERNING - Real agents have significant performance impact");
        } else {
            println!("  ‚ùå POOR - Real agents underperform significantly");
        }
    }

    fn print_metrics(&self, metrics: &BenchmarkMetrics, indent: &str) {
        println!("{}Total Duration: {:?}", indent, metrics.total_duration);
        println!("{}Issues Completed: {}", indent, metrics.issues_completed);
        println!("{}Issues Failed: {}", indent, metrics.issues_failed);
        println!(
            "{}Throughput: {:.2} issues/second",
            indent, metrics.throughput_issues_per_second
        );
        println!(
            "{}Average Time per Issue: {:?}",
            indent, metrics.average_time_per_issue
        );
        println!("{}P50 Time: {:?}", indent, metrics.p50_time_per_issue);
        println!("{}P90 Time: {:?}", indent, metrics.p90_time_per_issue);
        println!("{}P99 Time: {:?}", indent, metrics.p99_time_per_issue);
        println!("{}Memory Usage: {:.1} MB", indent, metrics.memory_usage_mb);
        println!("{}CPU Usage: {:.1}%", indent, metrics.cpu_usage_percent);
        println!("{}GitHub API Calls: {}", indent, metrics.github_api_calls);
        println!(
            "{}API Calls per Issue: {:.1}",
            indent, metrics.api_calls_per_issue
        );
        println!("{}Error Rate: {:.2}%", indent, metrics.error_rate * 100.0);
    }
}

#[cfg(test)]
mod benchmark_tests {
    use super::*;

    #[tokio::test]
    async fn test_mock_vs_real_performance_benchmark() {
        println!("üß™ Running mock vs real performance benchmark");

        let config = BenchmarkConfig {
            test_name: "Mock vs Real Agent Performance".to_string(),
            agent_count: 3,
            issues_per_agent: 4,
            timeout_per_issue: Duration::from_secs(30),
            warmup_iterations: 1,      // Reduced for testing
            measurement_iterations: 3, // Reduced for testing
        };

        let runner = BenchmarkRunner::new(config);
        let results = runner
            .run_benchmark()
            .await
            .expect("Benchmark should complete");

        // Validate results
        assert!(
            results.mock_results.issues_completed > 0,
            "Mock agents should complete issues"
        );
        assert!(
            results.real_results.issues_completed > 0,
            "Real agents should complete issues"
        );

        // Mock agents should be faster (lower latency)
        assert!(
            results.comparison.latency_ratio > 1.0,
            "Real agents should have higher latency than mocks"
        );

        // Real agents should use more resources
        assert!(
            results.real_results.memory_usage_mb > results.mock_results.memory_usage_mb,
            "Real agents should use more memory than mocks"
        );

        // Real agents should make more API calls
        assert!(
            results.real_results.api_calls_per_issue > results.mock_results.api_calls_per_issue,
            "Real agents should make more API calls than mocks"
        );

        // Overall performance should be reasonable
        assert!(
            results.comparison.overall_score > 0.1,
            "Real agents should have at least minimal performance compared to mocks"
        );

        println!("‚úÖ Mock vs real performance benchmark completed successfully");
    }

    #[tokio::test]
    async fn test_scalability_benchmark() {
        println!("üß™ Running scalability benchmark");

        let configurations = vec![
            ("Small Scale", 2, 3),
            ("Medium Scale", 4, 3),
            ("Large Scale", 6, 2), // Fewer issues per agent to keep test time reasonable
        ];

        let mut scalability_results = Vec::new();

        for (scale_name, agent_count, issues_per_agent) in configurations {
            println!(
                "Testing {} ({} agents, {} issues each)",
                scale_name, agent_count, issues_per_agent
            );

            let config = BenchmarkConfig {
                test_name: format!("Scalability - {}", scale_name),
                agent_count,
                issues_per_agent,
                timeout_per_issue: Duration::from_secs(30),
                warmup_iterations: 1,
                measurement_iterations: 2,
            };

            let runner = BenchmarkRunner::new(config);
            let results = runner
                .run_benchmark()
                .await
                .expect("Benchmark should complete");

            scalability_results.push((
                scale_name.to_string(),
                agent_count,
                results.real_results.throughput_issues_per_second,
            ));
        }

        // Analyze scalability
        println!("\nüìä Scalability Analysis:");
        for (scale_name, agent_count, throughput) in &scalability_results {
            println!(
                "  {} ({} agents): {:.2} issues/second",
                scale_name, agent_count, throughput
            );
        }

        // Check that throughput generally increases with agent count
        // (allowing for some variance due to test environment)
        let small_throughput = scalability_results[0].2;
        let large_throughput = scalability_results[2].2;

        assert!(
            large_throughput >= small_throughput * 0.8, // Allow some overhead
            "Scalability should improve with more agents"
        );

        println!("‚úÖ Scalability benchmark completed successfully");
    }

    #[tokio::test]
    async fn test_resource_efficiency_benchmark() {
        println!("üß™ Running resource efficiency benchmark");

        let config = BenchmarkConfig {
            test_name: "Resource Efficiency".to_string(),
            agent_count: 3,
            issues_per_agent: 5,
            timeout_per_issue: Duration::from_secs(30),
            warmup_iterations: 1,
            measurement_iterations: 3,
        };

        let runner = BenchmarkRunner::new(config);
        let results = runner
            .run_benchmark()
            .await
            .expect("Benchmark should complete");

        // Validate resource efficiency metrics
        assert!(
            results.mock_results.memory_usage_mb > 0.0,
            "Mock should have some memory usage"
        );
        assert!(
            results.real_results.memory_usage_mb > 0.0,
            "Real should have some memory usage"
        );

        // Real agents should be less resource efficient than mocks
        assert!(
            results.comparison.resource_efficiency_ratio < 1.0,
            "Real agents should use more resources than mocks"
        );

        // But resource usage should be reasonable
        assert!(
            results.real_results.memory_usage_mb < 500.0,
            "Real agents should not use excessive memory"
        );

        println!("Resource Efficiency Results:");
        println!(
            "  Mock Memory Usage: {:.1} MB",
            results.mock_results.memory_usage_mb
        );
        println!(
            "  Real Memory Usage: {:.1} MB",
            results.real_results.memory_usage_mb
        );
        println!(
            "  Efficiency Ratio: {:.2}",
            results.comparison.resource_efficiency_ratio
        );

        println!("‚úÖ Resource efficiency benchmark completed successfully");
    }

    #[tokio::test]
    async fn test_api_efficiency_benchmark() {
        println!("üß™ Running API efficiency benchmark");

        let config = BenchmarkConfig {
            test_name: "API Efficiency".to_string(),
            agent_count: 2,
            issues_per_agent: 6,
            timeout_per_issue: Duration::from_secs(30),
            warmup_iterations: 1,
            measurement_iterations: 3,
        };

        let runner = BenchmarkRunner::new(config);
        let results = runner
            .run_benchmark()
            .await
            .expect("Benchmark should complete");

        // Validate API efficiency metrics
        assert!(
            results.mock_results.github_api_calls > 0,
            "Mock should make API calls"
        );
        assert!(
            results.real_results.github_api_calls > 0,
            "Real should make API calls"
        );

        // Real agents should make more API calls than mocks
        assert!(
            results.real_results.api_calls_per_issue > results.mock_results.api_calls_per_issue,
            "Real agents should make more API calls per issue than mocks"
        );

        // But API usage should be reasonable
        assert!(
            results.real_results.api_calls_per_issue < 50.0,
            "Real agents should not make excessive API calls"
        );

        println!("API Efficiency Results:");
        println!(
            "  Mock API Calls per Issue: {:.1}",
            results.mock_results.api_calls_per_issue
        );
        println!(
            "  Real API Calls per Issue: {:.1}",
            results.real_results.api_calls_per_issue
        );
        println!(
            "  Efficiency Ratio: {:.2}",
            results.comparison.api_efficiency_ratio
        );

        println!("‚úÖ API efficiency benchmark completed successfully");
    }
}
